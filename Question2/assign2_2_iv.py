# -*- coding: utf-8 -*-
"""Assign2_2_iv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fWhfMUrzyt3STFtReiPeKPcWQJ-h2e30
"""

# Mounting the drive with the colab
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
# Importing the csv file to the colab.
url = '/content/drive/MyDrive/Colab Notebooks/A2Q2Data_train.csv'
df = pd.read_csv(url , header = None)
#dividing the data in x & y  with 100 and 1 features respectively
x = df.iloc[0:,0:100]
y = df.iloc[0:, 100:]

n_datapoints = df.shape[0]
n_features = df.shape[1] - 1

#splitting the data into 80 : 20 for training and testing.
import random
def TestTrainSplit(data, x, y):
  data = data.sample(n=10000)
  xtrain = data.iloc[0: 8000, 0:100]
  ytrain = data.iloc[0: 8000, 100]
  xtest = data.iloc[8000: , 0:100]
  ytest = data.iloc[8000:, 100]
  xtrain = np.array(xtrain)
  ytrain = np.array(ytrain)
  xtest = np.array(xtest)
  ytest = np.array(ytest)
  return xtrain, ytrain, xtest, ytest

n_features

def computeGradient(x, y,xtrain_txtrain,w, lammbda):
  # compute dw
  x_t = np.transpose(x)
  dw = xtrain_txtrain
  dw = np.matmul(dw, w)
  x_ty = np.matmul(x_t, y)
  lambdaw = np.multiply(lammbda, w)
  dw = np.subtract(dw, x_ty)
  dw = np.add(dw, lambdaw)
  dw = np.multiply(2, dw)
  return dw

def cost(x,y, w , n_datapoints):
  cost = 0
  xw = np.matmul(x, w)
  for i in range(n_datapoints):
    cost += (1/ 2*n_datapoints) * (xw[i][0] - y[i][0])**2
  return cost

def computeGradientDescent(n_iteration , x, y ,w, xtrain_txtrain,n_features, lammbda, alpha):
  for i in range(n_iteration):
    dw = computeGradient(x, y,xtrain_txtrain, w , lammbda)
    alpha_dw = np.multiply(alpha, dw) 
    w = np.subtract(w, alpha_dw)
  return w

def errorCost(x, y , w , n_datapoints):
  cost = 0
  xw = np.matmul(x, w)
  error = np.linalg.norm(y - xw)
  error = error ** 2
  return error

# Function to run the Ridge Linear Regression Algorithm 
def RidgeGD(data,x,y, n_datapoints, n_features , alpha, iter):
  w_list = []
  error_list = []
  minError = 10000000000
  minLambda = -1
  from sklearn.model_selection import train_test_split
  xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=4)
  # print(xtrain, xtest, ytrain, ytest)
  xtrain = np.array(xtrain)
  xtest = np.array(xtest)
  ytrain = np.array(ytrain)
  ytest = np.array(ytest)

  # xtrain, ytrain, xtest, ytest = TestTrainSplit(df, x, y)
  xtrain_t = np.transpose(xtrain)
  #computing the x^Tx separetely.
  xtrain_txtrain = np.matmul(xtrain_t, xtrain)
  for lammbda in range(0,iter):
    w = np.ones(n_features)
    w = computeGradientDescent(5000, xtrain, ytrain,w, xtrain_txtrain, n_features, lammbda, alpha)
    error = errorCost(xtest, ytest, w , 2000)
    print(lammbda, error)
    error_list.append(error)
    if minError > error:
      minError = error
      minLambda = lammbda
    w_list.append(w)
  return error_list , minLambda

##Plotting the graph between no of iterations and Error cost.
def PlotGraph(iter,errors):
  import matplotlib.pyplot as plt
  epochs =  range(iter)
  plt.xlabel('Value of Lambda')
  plt.ylabel('Error Cost')
  plt.title('Error Cost v/s Value of Lambda')
  plt.plot(epochs, errors)

#Running the Ridge algorithm with appropriate values
# alpha = 0.0000009
alpha = 0.0000007
iter = 30
errors, minLambda = RidgeGD(df, x, y, n_datapoints, n_features, alpha, iter)

#plotting the graph. 
print("Minimum Error occurs for the  value of Lambda is", minLambda)
PlotGraph(iter,errors)